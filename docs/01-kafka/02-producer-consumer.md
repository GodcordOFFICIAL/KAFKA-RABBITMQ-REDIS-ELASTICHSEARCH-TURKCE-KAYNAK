# ğŸš€ Kafka Producer & Consumer - Programmatic API KullanÄ±mÄ±

**Ã–zet**: Bu bÃ¶lÃ¼mde Kafka Producer ve Consumer API'lerini kullanarak programmatic olarak mesaj gÃ¶nderme ve alma iÅŸlemlerini Ã¶ÄŸreneceksiniz. Java, Python ve Node.js ile Ã§alÄ±ÅŸan Ã¶rnekler Ã¼zerinden serialization, partitioning, offset management ve error handling konularÄ±nÄ± derinlemesine inceleyeceÄŸiz.

## ğŸ¯ Learning Objectives

Bu bÃ¶lÃ¼mÃ¼ tamamladÄ±ÄŸÄ±nÄ±zda:
- [x] Kafka Producer API'sini kullanarak mesaj gÃ¶nderebileceksiniz
- [x] Kafka Consumer API'sini kullanarak mesaj alabileceksiniz
- [x] Serialization/Deserialization mekanizmalarÄ±nÄ± anlayacaksÄ±nÄ±z
- [x] Partitioning stratejilerini uygulayabileceksiniz
- [x] Error handling ve retry mekanizmalarÄ±nÄ± kullanabileceksiniz
- [x] Performance tuning parametrelerini ayarlayabileceksiniz

## ğŸ“‹ Prerequisites

- [x] [Kafka Temelleri](01-temeller.md) bÃ¶lÃ¼mÃ¼nÃ¼n tamamlanmasÄ±
- [x] Java 11+ veya Python 3.8+ veya Node.js 16+
- [x] Kafka cluster'Ä±n Ã§alÄ±ÅŸÄ±r durumda olmasÄ±
- [x] IDE kurulumu (IntelliJ IDEA, VS Code, etc.)

## ğŸ—ï¸ Producer Mimarisi

### Producer Ä°Ã§ YapÄ±sÄ±

```mermaid
graph LR
    A[Producer App] --> B[Serializer]
    B --> C[Partitioner]
    C --> D[RecordAccumulator]
    D --> E[Sender Thread]
    E --> F[Network Client]
    F --> G[Kafka Broker]
    
    D --> H[Batch Buffer]
    H --> E
    
    G --> I[Acknowledgment]
    I --> E
    E --> J[Callback]
    J --> A
```

**YapÄ±lan Ä°ÅŸlemler**:
- **Serialization**: Object'ler byte array'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
- **Partitioning**: MesajÄ±n hangi partition'a gideceÄŸi belirlenir
- **Batching**: Performance iÃ§in mesajlar batch'lenir
- **Network I/O**: Asenkron olarak broker'a gÃ¶nderilir
- **Acknowledgment**: Delivery confirmation alÄ±nÄ±r

### Producer Configuration

```java
// examples/kafka/java/src/main/java/ProducerConfig.java
Properties props = new Properties();

// Bootstrap servers - Broker baÄŸlantÄ± bilgileri
props.put("bootstrap.servers", "localhost:9092");

// Serializers - Key ve value serialization
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// Acknowledgment settings - Delivery guarantee
props.put("acks", "all");  // TÃ¼m replica'lardan ack bekle
props.put("retries", 3);   // Retry count
props.put("retry.backoff.ms", 1000);  // Retry delay

// Batching configuration - Performance optimization
props.put("batch.size", 16384);      // 16KB batch size
props.put("linger.ms", 10);          // Batch bekleme sÃ¼resi
props.put("buffer.memory", 33554432); // 32MB buffer

// Compression - Network efficiency
props.put("compression.type", "gzip");

// Idempotence - Duplicate prevention
props.put("enable.idempotence", true);
```

**Mimari AÃ§Ä±klamalar**:
- **acks=all**: GÃ¼venlik maksimum, performance orta
- **batch.size**: BÃ¼yÃ¼k batch = daha iyi throughput, daha yÃ¼ksek latency
- **linger.ms**: Batch'i bekletme sÃ¼resi, throughput vs latency trade-off
- **enable.idempotence**: Exactly-once delivery garantisi

## ğŸ’» Java Producer Implementasyonu

### Basit Producer

```java
// examples/kafka/java/src/main/java/SimpleProducer.java
package com.example.kafka;

import org.apache.kafka.clients.producer.*;
import java.util.Properties;
import java.util.concurrent.Future;

public class SimpleProducer {
    
    private final Producer<String, String> producer;
    private final String topicName;
    
    public SimpleProducer(String topicName) {
        this.topicName = topicName;
        
        // Producer yapÄ±landÄ±rmasÄ±
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringSerializer");
        
        // Delivery guarantee ayarlarÄ±
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        
        this.producer = new KafkaProducer<>(props);
    }
    
    /**
     * Senkron mesaj gÃ¶nderme - Blocking operation
     */
    public void sendMessageSync(String key, String value) {
        try {
            ProducerRecord<String, String> record = 
                new ProducerRecord<>(topicName, key, value);
            
            // Send iÅŸlemi ve metadata'yÄ± bekleme
            RecordMetadata metadata = producer.send(record).get();
            
            System.out.printf("Message sent successfully: " +
                "Topic=%s, Partition=%d, Offset=%d, Timestamp=%d%n",
                metadata.topic(), metadata.partition(), 
                metadata.offset(), metadata.timestamp());
                
        } catch (Exception e) {
            System.err.println("Error sending message: " + e.getMessage());
        }
    }
    
    /**
     * Asenkron mesaj gÃ¶nderme - Non-blocking operation
     */
    public void sendMessageAsync(String key, String value) {
        ProducerRecord<String, String> record = 
            new ProducerRecord<>(topicName, key, value);
        
        // Callback ile asenkron handling
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null) {
                    System.err.println("Failed to send message: " + exception.getMessage());
                } else {
                    System.out.printf("Message sent: Topic=%s, Partition=%d, Offset=%d%n",
                        metadata.topic(), metadata.partition(), metadata.offset());
                }
            }
        });
    }
    
    /**
     * Partition belirterek mesaj gÃ¶nderme
     */
    public void sendToSpecificPartition(String key, String value, int partition) {
        ProducerRecord<String, String> record = 
            new ProducerRecord<>(topicName, partition, key, value);
        
        producer.send(record, (metadata, exception) -> {
            if (exception != null) {
                System.err.println("Send failed: " + exception.getMessage());
            } else {
                System.out.printf("Sent to partition %d with offset %d%n",
                    metadata.partition(), metadata.offset());
            }
        });
    }
    
    /**
     * Producer'Ä± gÃ¼venli ÅŸekilde kapatma
     */
    public void close() {
        producer.close();
    }
    
    public static void main(String[] args) {
        SimpleProducer producer = new SimpleProducer("user-events");
        
        try {
            // Test mesajlarÄ± gÃ¶nderme
            for (int i = 0; i < 10; i++) {
                String key = "user-" + i;
                String value = "User event data: " + i;
                
                if (i % 2 == 0) {
                    producer.sendMessageSync(key, value);
                } else {
                    producer.sendMessageAsync(key, value);
                }
                
                Thread.sleep(1000); // 1 saniye bekleme
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        } finally {
            producer.close();
        }
    }
}
```

**YapÄ±lan Ä°ÅŸlemler**:
- **Senkron Send**: `producer.send().get()` ile blocking operation
- **Asenkron Send**: Callback function ile non-blocking operation
- **Partition Selection**: Explicit partition assignment
- **Error Handling**: Exception management ve retry logic

### Custom Partitioner

```java
// examples/kafka/java/src/main/java/CustomPartitioner.java
package com.example.kafka;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import java.util.Map;

public class CustomPartitioner implements Partitioner {
    
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, 
                        Object value, byte[] valueBytes, Cluster cluster) {
        
        int numPartitions = cluster.partitionCountForTopic(topic);
        
        if (key == null) {
            // Key yoksa round-robin daÄŸÄ±lÄ±m
            return (int) (Math.random() * numPartitions);
        }
        
        String keyString = key.toString();
        
        // Business logic based partitioning
        if (keyString.startsWith("premium_")) {
            // Premium kullanÄ±cÄ±lar partition 0'a gÃ¶nderilir
            return 0;
        } else if (keyString.startsWith("vip_")) {
            // VIP kullanÄ±cÄ±lar son partition'a gÃ¶nderilir
            return numPartitions - 1;
        } else {
            // DiÄŸer kullanÄ±cÄ±lar hash-based daÄŸÄ±lÄ±m
            return Math.abs(keyString.hashCode()) % numPartitions;
        }
    }
    
    @Override
    public void close() {
        // Cleanup resources if needed
    }
    
    @Override
    public void configure(Map<String, ?> configs) {
        // Configuration initialization
    }
}
```

**Mimari AÃ§Ä±klamalar**:
- **Business Logic Partitioning**: KullanÄ±cÄ± tipine gÃ¶re partition assignment
- **Load Distribution**: FarklÄ± partition'lara yÃ¼k daÄŸÄ±lÄ±mÄ±
- **Custom Logic**: Uygulamaya Ã¶zel partitioning stratejisi

## ğŸ” Consumer Mimarisi

### Consumer Ä°Ã§ YapÄ±sÄ±

```mermaid
graph LR
    A[Kafka Broker] --> B[Fetcher]
    B --> C[Deserializer]
    C --> D[Record Buffer]
    D --> E[Consumer App]
    
    E --> F[Offset Commit]
    F --> G[Coordinator]
    G --> A
    
    H[Consumer Group] --> I[Group Coordinator]
    I --> J[Partition Assignment]
    J --> B
```

**YapÄ±lan Ä°ÅŸlemler**:
- **Fetching**: Broker'dan message batch'leri Ã§ekilir
- **Deserialization**: Byte array'ler object'lere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
- **Offset Management**: Consumer position tracking
- **Group Coordination**: Partition assignment ve rebalancing

### Consumer Configuration

```java
// Consumer yapÄ±landÄ±rmasÄ±
Properties props = new Properties();

// Bootstrap servers
props.put("bootstrap.servers", "localhost:9092");

// Consumer group ayarlarÄ±
props.put("group.id", "user-event-processors");
props.put("group.instance.id", "consumer-1"); // Static membership

// Deserializers
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

// Offset management
props.put("auto.offset.reset", "earliest"); // latest, earliest, none
props.put("enable.auto.commit", false);     // Manuel commit control

// Session management
props.put("session.timeout.ms", 30000);     // 30 saniye
props.put("heartbeat.interval.ms", 3000);   // 3 saniye

// Fetch configuration
props.put("fetch.min.bytes", 1024);         // Minimum fetch size
props.put("fetch.max.wait.ms", 500);        // Maximum wait time
props.put("max.poll.records", 100);         // Records per poll
```

**Mimari AÃ§Ä±klamalar**:
- **auto.offset.reset**: Consumer ilk defa baÅŸladÄ±ÄŸÄ±nda hangi offset'ten baÅŸlayacaÄŸÄ±
- **enable.auto.commit**: Otomatik vs manuel offset commit kontrolÃ¼
- **session.timeout.ms**: Consumer group membership timeout'u

## ğŸ’» Java Consumer Implementasyonu

### Basit Consumer

```java
// examples/kafka/java/src/main/java/SimpleConsumer.java
package com.example.kafka;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import java.time.Duration;
import java.util.*;

public class SimpleConsumer {
    
    private final Consumer<String, String> consumer;
    private final String topicName;
    
    public SimpleConsumer(String groupId, String topicName) {
        this.topicName = topicName;
        
        // Consumer yapÄ±landÄ±rmasÄ±
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringDeserializer");
        
        // Offset management
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        
        // Performance tuning
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        
        this.consumer = new KafkaConsumer<>(props);
    }
    
    /**
     * Otomatik commit ile mesaj tÃ¼ketme
     */
    public void consumeWithAutoCommit() {
        consumer.subscribe(Arrays.asList(topicName));
        
        try {
            while (true) {
                // Polling - Broker'dan mesaj Ã§ekme
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(1000));
                
                // Her record'u iÅŸleme
                for (ConsumerRecord<String, String> record : records) {
                    processMessage(record);
                }
                
                // Auto commit enabled ise otomatik commit olur
            }
        } catch (Exception e) {
            System.err.println("Error in consumer: " + e.getMessage());
        } finally {
            consumer.close();
        }
    }
    
    /**
     * Manuel commit ile mesaj tÃ¼ketme - Daha gÃ¼venli
     */
    public void consumeWithManualCommit() {
        consumer.subscribe(Arrays.asList(topicName));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(1000));
                
                for (ConsumerRecord<String, String> record : records) {
                    try {
                        // MesajÄ± iÅŸle
                        processMessage(record);
                        
                        // BaÅŸarÄ±lÄ± iÅŸlem sonrasÄ± manuel commit
                        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                        offsets.put(
                            new TopicPartition(record.topic(), record.partition()),
                            new OffsetAndMetadata(record.offset() + 1)
                        );
                        consumer.commitSync(offsets);
                        
                    } catch (Exception e) {
                        System.err.println("Error processing message: " + e.getMessage());
                        // Hata durumunda commit yapÄ±lmaz, mesaj tekrar iÅŸlenir
                    }
                }
            }
        } catch (Exception e) {
            System.err.println("Error in consumer: " + e.getMessage());
        } finally {
            consumer.close();
        }
    }
    
    /**
     * Batch commit ile mesaj tÃ¼ketme - Performance optimized
     */
    public void consumeWithBatchCommit() {
        consumer.subscribe(Arrays.asList(topicName));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(1000));
                
                boolean allProcessedSuccessfully = true;
                
                // TÃ¼m mesajlarÄ± iÅŸle
                for (ConsumerRecord<String, String> record : records) {
                    try {
                        processMessage(record);
                    } catch (Exception e) {
                        System.err.println("Error processing message: " + e.getMessage());
                        allProcessedSuccessfully = false;
                        break; // Hata durumunda batch'i durdur
                    }
                }
                
                // TÃ¼m mesajlar baÅŸarÄ±lÄ± ise batch commit
                if (allProcessedSuccessfully && !records.isEmpty()) {
                    try {
                        consumer.commitSync(); // TÃ¼m partition'lar iÃ§in commit
                        System.out.println("Batch committed successfully: " + records.count() + " records");
                    } catch (CommitFailedException e) {
                        System.err.println("Commit failed: " + e.getMessage());
                    }
                }
            }
        } catch (Exception e) {
            System.err.println("Error in consumer: " + e.getMessage());
        } finally {
            consumer.close();
        }
    }
    
    /**
     * Belirli partition'dan mesaj tÃ¼ketme
     */
    public void consumeFromSpecificPartition(int partition) {
        TopicPartition topicPartition = new TopicPartition(topicName, partition);
        consumer.assign(Arrays.asList(topicPartition));
        
        // Belirli offset'ten baÅŸlatma (opsiyonel)
        consumer.seekToBeginning(Arrays.asList(topicPartition));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(1000));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Partition %d, Offset %d: %s = %s%n",
                        record.partition(), record.offset(), record.key(), record.value());
                }
            }
        } finally {
            consumer.close();
        }
    }
    
    /**
     * Mesaj iÅŸleme business logic'i
     */
    private void processMessage(ConsumerRecord<String, String> record) {
        // Business logic implementation
        System.out.printf("Processing message: Key=%s, Value=%s, " +
            "Partition=%d, Offset=%d, Timestamp=%d%n",
            record.key(), record.value(), record.partition(), 
            record.offset(), record.timestamp());
        
        // Simulate processing time
        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }
    
    public static void main(String[] args) {
        SimpleConsumer consumer = new SimpleConsumer("test-group", "user-events");
        
        // FarklÄ± commit stratejilerini test edebilirsiniz
        consumer.consumeWithManualCommit();
    }
}
```

**YapÄ±lan Ä°ÅŸlemler**:
- **Auto Commit**: Otomatik offset commit, basit ama risk
- **Manual Commit**: Her mesaj sonrasÄ± commit, gÃ¼venli ama yavaÅŸ
- **Batch Commit**: Batch sonrasÄ± commit, performance optimized
- **Specific Partition**: Belirli partition'dan okuma

## ğŸ Python Producer & Consumer

### Python Producer

```python
# examples/kafka/python/simple_producer.py
from kafka import KafkaProducer
import json
import time
from typing import Dict, Any, Optional

class SimpleProducer:
    """
    Kafka Producer wrapper class
    """
    
    def __init__(self, bootstrap_servers: str = 'localhost:9092'):
        """
        Producer initialization
        """
        self.producer = KafkaProducer(
            # Bootstrap servers
            bootstrap_servers=[bootstrap_servers],
            
            # Serialization - JSON kullanarak
            key_serializer=lambda x: x.encode('utf-8') if x else None,
            value_serializer=lambda x: json.dumps(x).encode('utf-8'),
            
            # Delivery guarantee
            acks='all',  # TÃ¼m replica'lardan ack bekle
            retries=3,   # Retry sayÄ±sÄ±
            retry_backoff_ms=1000,  # Retry delay
            
            # Performance tuning
            batch_size=16384,      # 16KB batch size
            linger_ms=10,          # Batch bekleme sÃ¼resi
            buffer_memory=33554432,  # 32MB buffer
            
            # Compression
            compression_type='gzip',
            
            # Idempotence
            enable_idempotence=True
        )
    
    def send_message_sync(self, topic: str, key: str, value: Dict[str, Any]) -> bool:
        """
        Senkron mesaj gÃ¶nderme - Blocking operation
        """
        try:
            # Send ve result'Ä± bekle
            future = self.producer.send(topic, key=key, value=value)
            record_metadata = future.get(timeout=10)
            
            print(f"Message sent successfully:")
            print(f"  Topic: {record_metadata.topic}")
            print(f"  Partition: {record_metadata.partition}")
            print(f"  Offset: {record_metadata.offset}")
            print(f"  Timestamp: {record_metadata.timestamp}")
            
            return True
            
        except Exception as e:
            print(f"Error sending message: {e}")
            return False
    
    def send_message_async(self, topic: str, key: str, value: Dict[str, Any]) -> None:
        """
        Asenkron mesaj gÃ¶nderme - Non-blocking operation
        """
        def on_send_success(record_metadata):
            print(f"Message sent to {record_metadata.topic}[{record_metadata.partition}] "
                  f"at offset {record_metadata.offset}")
        
        def on_send_error(excp):
            print(f"Failed to send message: {excp}")
        
        # Asenkron send
        future = self.producer.send(topic, key=key, value=value)
        future.add_callback(on_send_success)
        future.add_errback(on_send_error)
    
    def send_to_partition(self, topic: str, partition: int, 
                         key: str, value: Dict[str, Any]) -> bool:
        """
        Belirli partition'a mesaj gÃ¶nderme
        """
        try:
            future = self.producer.send(topic, key=key, value=value, partition=partition)
            record_metadata = future.get(timeout=10)
            
            print(f"Message sent to partition {record_metadata.partition} "
                  f"with offset {record_metadata.offset}")
            
            return True
            
        except Exception as e:
            print(f"Error sending to partition: {e}")
            return False
    
    def close(self):
        """
        Producer'Ä± gÃ¼venli ÅŸekilde kapatma
        """
        self.producer.close()

# Test usage
if __name__ == "__main__":
    producer = SimpleProducer()
    
    try:
        # Test mesajlarÄ± gÃ¶nderme
        for i in range(10):
            message = {
                'user_id': f'user_{i}',
                'event_type': 'page_view',
                'timestamp': int(time.time()),
                'data': {
                    'page': f'/page_{i}',
                    'duration': i * 100
                }
            }
            
            key = f"user_{i}"
            
            # Senkron ve asenkron gÃ¶nderim test
            if i % 2 == 0:
                producer.send_message_sync('user-events', key, message)
            else:
                producer.send_message_async('user-events', key, message)
            
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("Stopping producer...")
    finally:
        producer.close()
```

### Python Consumer

```python
# examples/kafka/python/simple_consumer.py
from kafka import KafkaConsumer, TopicPartition
import json
from typing import Dict, Any, List, Optional

class SimpleConsumer:
    """
    Kafka Consumer wrapper class
    """
    
    def __init__(self, group_id: str, bootstrap_servers: str = 'localhost:9092'):
        """
        Consumer initialization
        """
        self.consumer = KafkaConsumer(
            # Bootstrap servers
            bootstrap_servers=[bootstrap_servers],
            
            # Consumer group
            group_id=group_id,
            
            # Deserialization
            key_deserializer=lambda x: x.decode('utf-8') if x else None,
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            
            # Offset management
            auto_offset_reset='earliest',  # earliest, latest, none
            enable_auto_commit=False,      # Manuel commit control
            
            # Session management
            session_timeout_ms=30000,      # 30 saniye
            heartbeat_interval_ms=3000,    # 3 saniye
            
            # Fetch configuration
            fetch_min_bytes=1024,          # Minimum fetch size
            fetch_max_wait_ms=500,         # Maximum wait time
            max_poll_records=100           # Records per poll
        )
    
    def consume_with_auto_commit(self, topics: List[str]) -> None:
        """
        Otomatik commit ile mesaj tÃ¼ketme
        """
        # Auto commit'i enable et
        self.consumer._config['enable_auto_commit'] = True
        
        # Topic'lere subscribe ol
        self.consumer.subscribe(topics)
        
        try:
            for message in self.consumer:
                self.process_message(message)
                # Auto commit enabled olduÄŸu iÃ§in otomatik commit
                
        except KeyboardInterrupt:
            print("Stopping consumer...")
        finally:
            self.consumer.close()
    
    def consume_with_manual_commit(self, topics: List[str]) -> None:
        """
        Manuel commit ile mesaj tÃ¼ketme - Daha gÃ¼venli
        """
        self.consumer.subscribe(topics)
        
        try:
            while True:
                # Poll messages
                message_batch = self.consumer.poll(timeout_ms=1000)
                
                for topic_partition, messages in message_batch.items():
                    for message in messages:
                        try:
                            # MesajÄ± iÅŸle
                            self.process_message(message)
                            
                            # BaÅŸarÄ±lÄ± iÅŸlem sonrasÄ± manuel commit
                            self.consumer.commit({
                                topic_partition: message.offset + 1
                            })
                            
                        except Exception as e:
                            print(f"Error processing message: {e}")
                            # Hata durumunda commit yapÄ±lmaz
                            
        except KeyboardInterrupt:
            print("Stopping consumer...")
        finally:
            self.consumer.close()
    
    def consume_with_batch_commit(self, topics: List[str]) -> None:
        """
        Batch commit ile mesaj tÃ¼ketme - Performance optimized
        """
        self.consumer.subscribe(topics)
        
        try:
            while True:
                # Poll messages
                message_batch = self.consumer.poll(timeout_ms=1000)
                
                if not message_batch:
                    continue
                
                all_processed_successfully = True
                processed_count = 0
                
                # TÃ¼m mesajlarÄ± iÅŸle
                for topic_partition, messages in message_batch.items():
                    for message in messages:
                        try:
                            self.process_message(message)
                            processed_count += 1
                        except Exception as e:
                            print(f"Error processing message: {e}")
                            all_processed_successfully = False
                            break
                    
                    if not all_processed_successfully:
                        break
                
                # TÃ¼m mesajlar baÅŸarÄ±lÄ± ise batch commit
                if all_processed_successfully and processed_count > 0:
                    try:
                        self.consumer.commit()
                        print(f"Batch committed successfully: {processed_count} records")
                    except Exception as e:
                        print(f"Commit failed: {e}")
                        
        except KeyboardInterrupt:
            print("Stopping consumer...")
        finally:
            self.consumer.close()
    
    def consume_from_specific_partition(self, topic: str, partition: int) -> None:
        """
        Belirli partition'dan mesaj tÃ¼ketme
        """
        topic_partition = TopicPartition(topic, partition)
        self.consumer.assign([topic_partition])
        
        # Belirli offset'ten baÅŸlatma (opsiyonel)
        self.consumer.seek_to_beginning(topic_partition)
        
        try:
            for message in self.consumer:
                print(f"Partition {message.partition}, Offset {message.offset}: "
                      f"{message.key} = {message.value}")
        except KeyboardInterrupt:
            print("Stopping consumer...")
        finally:
            self.consumer.close()
    
    def process_message(self, message) -> None:
        """
        Mesaj iÅŸleme business logic'i
        """
        print(f"Processing message:")
        print(f"  Key: {message.key}")
        print(f"  Value: {message.value}")
        print(f"  Partition: {message.partition}")
        print(f"  Offset: {message.offset}")
        print(f"  Timestamp: {message.timestamp}")
        print("-" * 50)
        
        # Simulate processing time
        import time
        time.sleep(0.1)

# Test usage
if __name__ == "__main__":
    consumer = SimpleConsumer("python-test-group")
    
    # FarklÄ± commit stratejilerini test edebilirsiniz
    consumer.consume_with_manual_commit(['user-events'])
```

**YapÄ±lan Ä°ÅŸlemler**:
- **JSON Serialization**: Python dict'leri JSON olarak serialize edilir
- **Error Handling**: Try-catch bloklarÄ± ile hata yÃ¶netimi
- **Flexible Commit**: FarklÄ± commit stratejileri
- **Type Hints**: Code readability iÃ§in type annotations

## ğŸ¯ Hands-on Lab: E-commerce Event System

### Lab Hedefi
GerÃ§ek zamanlÄ± bir e-commerce event sistemi oluÅŸturacaÄŸÄ±z:
- **Order Events**: SipariÅŸ oluÅŸturma, gÃ¼ncelleme, iptal
- **User Events**: KullanÄ±cÄ± aktiviteleri, profil deÄŸiÅŸiklikleri
- **Inventory Events**: Stok gÃ¼ncellemeleri

### AdÄ±m 1: Topic'leri OluÅŸturalÄ±m

```bash
# Order events topic
docker exec -it kafka1 kafka-topics.sh \
    --create --bootstrap-server localhost:9092 \
    --topic order-events --partitions 3 --replication-factor 1

# User events topic  
docker exec -it kafka1 kafka-topics.sh \
    --create --bootstrap-server localhost:9092 \
    --topic user-events --partitions 2 --replication-factor 1

# Inventory events topic
docker exec -it kafka1 kafka-topics.sh \
    --create --bootstrap-server localhost:9092 \
    --topic inventory-events --partitions 2 --replication-factor 1
```

### AdÄ±m 2: Order Service Producer

```java
// examples/kafka/java/src/main/java/OrderService.java
public class OrderService {
    private final Producer<String, String> producer;
    
    public void createOrder(String userId, String productId, int quantity) {
        // Order event oluÅŸtur
        OrderEvent orderEvent = new OrderEvent(
            UUID.randomUUID().toString(),
            userId, 
            productId, 
            quantity, 
            "CREATED",
            System.currentTimeMillis()
        );
        
        // User ID key olarak kullan (same user same partition)
        producer.send(new ProducerRecord<>("order-events", userId, 
            orderEvent.toJson()));
    }
}
```

### AdÄ±m 3: Inventory Service Consumer

```python
# examples/kafka/python/inventory_service.py
class InventoryService:
    def __init__(self):
        self.consumer = SimpleConsumer("inventory-service")
        
    def process_order_events(self):
        self.consumer.subscribe(['order-events'])
        
        for message in self.consumer:
            order_event = json.loads(message.value)
            
            if order_event['status'] == 'CREATED':
                self.reserve_inventory(order_event)
            elif order_event['status'] == 'CANCELLED':
                self.release_inventory(order_event)
```

## âœ… Checklist - Producer & Consumer

### Producer Beceriler
- [ ] Synchronous vs asynchronous sending arasÄ±ndaki farkÄ± anlÄ±yorum
- [ ] Serialization/deserialization mekanizmalarÄ±nÄ± kullanabiliyorum
- [ ] Custom partitioner yazabiliyorum
- [ ] Producer configuration parametrelerini ayarlayabiliyorum
- [ ] Error handling ve retry mekanizmalarÄ±nÄ± uygulayabiliyorum

### Consumer Beceriler
- [ ] Different commit strategies arasÄ±ndaki farklarÄ± anlÄ±yorum
- [ ] Consumer group rebalancing mekanizmasÄ±nÄ± kavradÄ±m
- [ ] Offset management'Ä± manuel olarak yapabiliyorum
- [ ] Specific partition'dan okuma yapabiliyorum
- [ ] Consumer lag'i monitÃ¶r edebiliyorum

### Performance Tuning
- [ ] Batch size ve linger.ms parametrelerini optimize edebiliyorum
- [ ] Compression algoritmalarÄ± kullanabiliyorum
- [ ] Memory ve network ayarlarÄ±nÄ± yapabiliyorum
- [ ] Throughput vs latency trade-off'unu anlÄ±yorum

## ğŸš« Common Mistakes ve Ã‡Ã¶zÃ¼mleri

### 1. **Auto Commit KullanÄ±mÄ±nda At-Least-Once Garanti KaybÄ±**
```java
// YANLIÅ: Auto commit enabled, processing exception sonrasÄ± mesaj kaybolur
props.put("enable.auto.commit", true);

// DOÄRU: Manuel commit, processing sonrasÄ± commit
props.put("enable.auto.commit", false);
consumer.commitSync(); // Her baÅŸarÄ±lÄ± processing sonrasÄ±
```

### 2. **Producer Buffer Overflow**
```java
// YANLIÅ: Buffer dolduÄŸunda blocking
props.put("buffer.memory", 1024); // Ã‡ok kÃ¼Ã§Ã¼k

// DOÄRU: Yeterli buffer space
props.put("buffer.memory", 33554432); // 32MB
props.put("max.block.ms", 5000); // Timeout protection
```

### 3. **Consumer Poll Timeout**
```java
// YANLIÅ: Ã‡ok kÄ±sa timeout
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

// DOÄRU: Reasonable timeout
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
```

## ğŸ† Solutions / Hints

### Lab Ã‡Ã¶zÃ¼mÃ¼ Ä°puÃ§larÄ±

1. **Order Events GÃ¶rÃ¼nmÃ¼yor**:
   - Topic'in doÄŸru oluÅŸturulduÄŸunu kontrol edin
   - Producer ve consumer'Ä±n aynÄ± topic'i kullandÄ±ÄŸÄ±ndan emin olun
   - Kafka UI'da mesajlarÄ± kontrol edin

2. **Consumer Lag ArtÄ±yor**:
   - Processing time'Ä± optimize edin
   - Consumer sayÄ±sÄ±nÄ± artÄ±rÄ±n
   - Batch size'Ä± ayarlayÄ±n

3. **Duplicate Messages**:
   - Enable idempotence kullanÄ±n
   - Manuel commit stratejisi uygulayÄ±n
   - Unique message ID'leri kullanÄ±n

## ğŸš€ Sonraki AdÄ±mlar

Bu bÃ¶lÃ¼mÃ¼ tamamladÄ±ktan sonra:

1. **[Topic & Partition Management](03-topic-partition.md)** - Ä°leri seviye topic yÃ¶netimi
2. **[Event Sourcing](04-event-sourcing.md)** - Event-driven architecture
3. **[Kafka Connect](05-kafka-connect.md)** - Veri entegrasyonu

## ğŸ“š Ek Kaynaklar

- [Kafka Producer API Docs](https://kafka.apache.org/documentation/#producerapi)
- [Kafka Consumer API Docs](https://kafka.apache.org/documentation/#consumerapi)
- [Kafka Client Performance Tuning](https://kafka.apache.org/documentation/#producerconfigs)

---

**Tebrikler! ğŸ‰** Producer ve Consumer API'lerini baÅŸarÄ±yla Ã¶ÄŸrendiniz. ArtÄ±k programmatic olarak Kafka ile mesajlaÅŸma yapabiliyorsunuz.